{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPxCx7eXDH4Qq+xMxO+vMlq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Generate Candidates #\n","We take candidates from a range of sources:\n","* Items already interacted with in the session\n","* Our covisitation matrices\n","* A word2vec model\n","* An ALS recommender\n"],"metadata":{"id":"1SdiYblJx2ao"}},{"cell_type":"code","source":["local = True\n","if local:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  %cd /content/drive/MyDrive/'Kaggle Otto Reccommender'/data\n","  path_to_module = '/content/drive/MyDrive/Kaggle Otto Reccommender/'\n","else:\n","  !mkdir /my_mnt_dir\n","  !google-drive-ocamlfuse /my_mnt_dir\n","  %cd /my_mnt_dir/'Kaggle Otto Reccommender'/data\n","  path_to_module = '/my_mnt_dir/Kaggle Otto Reccommender/'\n","\n","import sys    \n","sys.path.append(path_to_module)"],"metadata":{"id":"rgw3DUBDx7hN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install implicit\n","!pip install Annoy\n","!pip install fastparquet"],"metadata":{"id":"M9QNAWgIugYv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","import numpy as np\n","import pandas as pd\n","import gc\n","import os\n","import seaborn as sns\n","from otto_utils import get_train, get_test, convert_columns, save_parquet, make_directory, create_sub\n","from tqdm import tqdm\n","import scipy.sparse as sps\n","import implicit\n","from gensim.test.utils import common_texts\n","from gensim.models import Word2Vec\n","from annoy import AnnoyIndex\n"],"metadata":{"id":"bbpCdPcQOMqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_prop = None\n","validation = True\n","covisitation = True\n","cart_order = True\n","also_buy = True\n","als = True\n","word2vec = True\n","\n","path_to_candidate_features = './train_candidate_features' if validation else './test_candidate_features'\n","n=20"],"metadata":{"id":"QZ6WwCocB3CD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reduced_df = get_test(validation, sample_prop)\n","reduced_df.rename(columns = {'type' : 'source'}, inplace=True)\n","reduced_df['joiner'] = 1"],"metadata":{"id":"f4EnH3CgZf89"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculate the candidates from items already in the basket ##"],"metadata":{"id":"5soINLMjD-cK"}},{"cell_type":"code","source":["training_skeleton = reduced_df\n","training_skeleton['time_weight'] = training_skeleton.groupby('session')['aid'].transform(lambda x: np.logspace(0.1, 1, x.shape[0], base=2, endpoint=True)) - 1 # Changing this from 0.1 to 0.5 impacts recall by 0.01\n","training_skeleton['type_weight'] = 1\n","training_skeleton.loc[training_skeleton.source == 'carts', 'type_weight'] = 3\n","training_skeleton.loc[training_skeleton.source == 'orders', 'type_weight'] = 6\n","training_skeleton['weight'] = training_skeleton['time_weight'] * training_skeleton['type_weight']\n","training_skeleton = training_skeleton.groupby(['session', 'aid'], as_index=False).agg({'weight' : 'sum'})\n","training_skeleton.sort_values(by=['session', 'weight'], ascending=[True, False], inplace=True)\n","training_skeleton['n_basket'] = training_skeleton.groupby('session').cumcount() + 1\n","training_skeleton = convert_columns(training_skeleton)\n","\n","save_parquet(training_skeleton, f'{path_to_candidate_features}/basket', files=100, split_column = 'session')"],"metadata":{"id":"tmAKeMmvTK5K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculate the candidates from the covisitation matrices ##"],"metadata":{"id":"iMYwiBkVEB_x"}},{"cell_type":"code","source":["if covisitation:\n","  files = glob.glob(f'{path_to_candidate_features}/covisitation_parquet/wgt_*_top20*')\n","  covisitation_matrix = convert_columns(pd.read_parquet(files))\n","  for column in ['aid_x', 'aid_y']:\n","    covisitation_matrix[column] = covisitation_matrix[column].astype('int32')\n","\n","  sessions = reduced_df['session'].unique()\n","  sessions.sort()\n","  session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), 10 ) ]\n","\n","  covisitation_list = []\n","  for i, session_list in enumerate(tqdm(session_lists)):\n","    chunk = reduced_df.loc[(reduced_df['session'] >= min(session_list)) & (reduced_df['session'] <= max(session_list))]\n","    covisitation_options = (\n","        chunk.merge(\n","            covisitation_matrix,\n","            how='left',\n","            left_on = ['aid'],\n","            right_on = ['aid_x']).drop_duplicates(subset=['session', 'aid_x', 'aid_y'], keep='last')\n","        .groupby(['session', 'aid_y'], as_index=False)\n","        .agg({'ts': 'max', 'aid' : 'count', 'pairings' : 'sum'}) \n","        .sort_values(by=['session', 'aid', 'ts', 'pairings'], ascending=[True, False, False, False])\n","        .drop(columns={'aid', 'ts'})\n","        .rename(columns={'aid_y' : 'aid'})\n","    )\n","    covisitation_options['n'] = covisitation_options.groupby('session').cumcount() + 1\n","    covisitation_options = covisitation_options.loc[covisitation_options['n'] <= 150]\n","\n","    #covisitation_options.drop(columns='n', inplace=True)\n","    for column in ['aid', 'session']:\n","      covisitation_options[column] = covisitation_options[column].astype('int32')\n","    covisitation_list.append(covisitation_options)\n","    del chunk\n","\n","  covisitation_options = pd.concat(covisitation_list)\n","  del covisitation_list\n","\n","  save_parquet(covisitation_options, f'{path_to_candidate_features}/covisitation', files=100, split_column = 'session')\n","\n","  del covisitation_options"],"metadata":{"id":"QiYVCoU1Z-I7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if cart_order:\n","  files = glob.glob(f'{path_to_candidate_features}/cart_order_parquet/*')\n","  covisitation_matrix = convert_columns(pd.read_parquet(files))\n","  for column in ['aid_x', 'aid_y']:\n","    covisitation_matrix[column] = covisitation_matrix[column].astype('int32')\n","\n","  sessions = reduced_df['session'].unique()\n","  sessions.sort()\n","  session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), 10 ) ]\n","\n","  covisitation_list = []\n","  for i, session_list in enumerate(tqdm(session_lists)):\n","    chunk = reduced_df.loc[(reduced_df['session'] >= min(session_list)) & (reduced_df['session'] <= max(session_list))]\n","    covisitation_options = (\n","        chunk.merge(\n","            covisitation_matrix,\n","            how='left',\n","            left_on = ['aid'],\n","            right_on = ['aid_x']).drop_duplicates(subset=['session', 'aid_x', 'aid_y'], keep='last')\n","        .groupby(['session', 'aid_y'], as_index=False)\n","        .agg({'ts': 'max', 'aid' : 'count', 'pairings' : 'sum'})\n","        .sort_values(by=['session', 'aid', 'ts', 'pairings'], ascending=[True, False, False, False])\n","        .drop(columns={'aid', 'ts'})\n","        .rename(columns={'aid_y' : 'aid'})\n","    )\n","    covisitation_options['n'] = covisitation_options.groupby('session').cumcount() + 1\n","    covisitation_options = covisitation_options.loc[covisitation_options['n'] <= 150]\n","\n","    #covisitation_options.drop(columns='n', inplace=True)\n","    for column in ['aid', 'session']:\n","      covisitation_options[column] = covisitation_options[column].astype('int32')\n","    covisitation_list.append(covisitation_options)\n","    del chunk\n","\n","  covisitation_options = pd.concat(covisitation_list)\n","  del covisitation_list\n","\n","  save_parquet(covisitation_options, f'{path_to_candidate_features}/cart_order', files=100, split_column = 'session')\n","\n","  del covisitation_options\n"],"metadata":{"id":"PJcysKJf3Q3F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if also_buy:\n","  files = glob.glob(f'{path_to_candidate_features}/also_buy_parquet/*')\n","  covisitation_matrix = convert_columns(pd.read_parquet(files))\n","  for column in ['aid_x', 'aid_y']:\n","    covisitation_matrix[column] = covisitation_matrix[column].astype('int32')\n","\n","  sessions = reduced_df['session'].unique()\n","  sessions.sort()\n","  session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), 10 ) ]\n","\n","  covisitation_list = []\n","  for i, session_list in enumerate(tqdm(session_lists)):\n","    chunk = reduced_df.loc[\n","        (reduced_df['session'] >= min(session_list))\n","        & (reduced_df['session'] <= max(session_list))\n","        & (reduced_df['source'].isin(['carts', 'orders']))]\n","    covisitation_options = (\n","        chunk.merge(\n","            covisitation_matrix,\n","            how='left',\n","            left_on = ['aid'],\n","            right_on = ['aid_x']).drop_duplicates(subset=['session', 'aid_x', 'aid_y'], keep='last')\n","        .groupby(['session', 'aid_y'], as_index=False)\n","        .agg({'ts': 'max', 'aid' : 'count', 'pairings' : 'sum'})\n","        .sort_values(by=['session', 'aid', 'ts', 'pairings'], ascending=[True, False, False, False])\n","        .drop(columns={'aid', 'ts'})\n","        .rename(columns={'aid_y' : 'aid'})\n","    )\n","    covisitation_options['n'] = covisitation_options.groupby('session').cumcount() + 1\n","    covisitation_options = covisitation_options.loc[covisitation_options['n'] <= 150]\n","\n","    #covisitation_options.drop(columns='n', inplace=True)\n","    for column in ['aid', 'session']:\n","      covisitation_options[column] = covisitation_options[column].astype('int32')\n","    covisitation_list.append(covisitation_options)\n","    del chunk\n","\n","  covisitation_options = pd.concat(covisitation_list)\n","  del covisitation_list\n","\n","  save_parquet(covisitation_options, f'{path_to_candidate_features}/also_buy', files=100, split_column = 'session')\n","  del covisitation_options"],"metadata":{"id":"OFZUcPVq6v6Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create the word2vec candidates"],"metadata":{"id":"jzQg51HBDny_"}},{"cell_type":"code","source":["## Word2Vec functions:\n","def get_session_vector(df, w2vec):\n","  aids = df.aid.unique()\n","  for i, aid in enumerate(aids):\n","    vec = w2vec.wv[aid] if i == 0 else vec + w2vec.wv[aid]\n","  vec = vec / len(aids)\n","  return vec\n","\n","def get_close_aids(df, w2vec, index, idx2aid, n=20):\n","  session_vec = get_session_vector(df, w2vec)\n","  close_aids = get_nearest_neighbours(session_vec, index, idx2aid, n)\n","  return close_aids\n","\n","def get_nearest_neighbours(x, index, idx2aid, n=20):\n","  indexes, distances = index.get_nns_by_vector(x, n, search_k=-1, include_distances=True)\n","  aids = [idx2aid[i] for i in indexes]\n","  df = pd.DataFrame(data={'aid' : aids, 'w2vec_dist' : distances})\n","  return df\n","\n","def get_word2vec_recs(train, test, n=20):\n","  vector_size = 32\n","  epochs = 9\n","  sg = 1\n","  pop_thresh = 0.82415\n","  window = 8\n","  distance = 'angular'\n","\n","  reduced_df = pd.concat([train, test[['session','aid']]])\n","  del train\n","  sentences = reduced_df.groupby('session', as_index=False).agg({'aid' : lambda x: [str(i) for i in x.tolist()]}).rename(columns={'aid' : 'sentence'})\n","  sentences = sentences['sentence'].to_list()\n","\n","  w2vec = Word2Vec(sentences=sentences, size=vector_size, iter = epochs, sg=sg, min_count=1, workers=14, window=window)\n","\n","  index = AnnoyIndex(vector_size, distance)\n","  aid2idx = {}\n","\n","  popular_aids = test.groupby('aid', as_index=False).agg({'session' : 'count'})\n","  popular_aids = popular_aids.loc[popular_aids['session'] > popular_aids['session'].quantile(pop_thresh)]\n","  popular_aid_list = popular_aids.aid.unique()\n","\n","  for i, aid in enumerate(popular_aid_list):\n","    aid = str(aid)\n","    aid2idx[aid] = i\n","    index.add_item(i, w2vec.wv[aid])\n","  idx2aid = { v : k for k, v in aid2idx.items()}\n","  index.build(40)\n","\n","  reduced_test = test.copy()\n","  reduced_test['aid'] = reduced_test['aid'].astype('str')\n","  reduced_test['aid_vector'] = reduced_test['aid'].apply(lambda x: w2vec.wv[x])\n","\n","  reduced_test = reduced_test.groupby('session').apply(lambda x: get_close_aids(x, w2vec, index, idx2aid, n)).reset_index().drop(columns='level_1')\n","  reduced_test['aid'] = reduced_test['aid'].astype('int32')\n","  reduced_test['n'] = reduced_test.groupby('session').cumcount() + 1\n","\n","  return reduced_test"],"metadata":{"id":"uclzXsiR-xUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if word2vec:\n","  train = get_train(validation, sample_prop)\n","  word2vec_recs = get_word2vec_recs(train, reduced_df, 100)\n","  \n","  save_parquet(word2vec_recs, f'{path_to_candidate_features}/word2vec', files=100, split_column = 'session')"],"metadata":{"id":"qILPphPwN7au"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create the ALS candidates"],"metadata":{"id":"RIBjso6IDq4T"}},{"cell_type":"code","source":["## ALS functions\n","def get_items_to_exclude(reduced_df, proportion=0):\n","  ''' returns items with low popularity in the test set to exclude from predictions '''\n","  items_to_exclude = reduced_df.loc[reduced_df['dataset'] == 'test'].groupby('item_id', as_index=False).agg({'test_set_actions' : 'sum'})\n","  n = items_to_exclude['test_set_actions'].quantile(proportion)\n","  items_to_exclude = items_to_exclude.loc[items_to_exclude['test_set_actions'] <= n]\n","  items_to_exclude = items_to_exclude['item_id'].tolist()\n","  return items_to_exclude\n","\n","def get_users_to_keep(reduced_df, n=0):\n","  ''' get a list of all user codes with total interactions >= n '''\n","  users_to_keep = reduced_df.groupby('user_id', as_index=False).agg({'aid' : 'count'})\n","  users_to_keep = users_to_keep.loc[users_to_keep['aid'] >= n]\n","  users_to_keep = users_to_keep.user_id.tolist()\n","  return users_to_keep\n","\n","def get_als_recommendations(train, test, n_recs=20):\n","  iterations = 2\n","  factors = 800\n","  regularization = 1.7050\n","  minimum_clicks = 22\n","  popularity_threshold = 0.10\n","\n","  train['dataset'] = 'train'\n","  test['dataset'] = 'test'\n","  reduced_df = pd.concat([train, test])\n","  del train\n","\n","  reduced_df.reset_index(inplace=True)\n","\n","  reduced_df['user'] = reduced_df['session'].astype('category')\n","  reduced_df['user_id'] = reduced_df['user'].cat.codes\n","  reduced_df['item'] = reduced_df['aid'].astype('category')\n","  reduced_df['item_id'] = reduced_df['item'].cat.codes\n","  reduced_df['test_set_actions'] = 0\n","  reduced_df.loc[reduced_df['dataset'] == 'test', 'test_set_actions'] = 1\n","  reduced_df = convert_columns(reduced_df)\n","\n","  test_indices_start = len(reduced_df.loc[reduced_df['dataset'] == 'train'].session.unique())\n","  test_indices_end = len(reduced_df.session.unique())\n","  item_ids = {k: v for k, v in zip(reduced_df['item_id'], reduced_df['item'])}\n","  validation_user_ids = [id for id in range(test_indices_start, test_indices_end)]\n","  reduced_df.drop(columns=['user', 'ts', 'index','item'], inplace=True)\n","\n","  user_item = sps.coo_matrix(\n","      (np.ones(reduced_df.shape[0]), # We're using a matrix of ones, but using type weights or repurchase weights could help!\n","      (reduced_df['user_id'],\n","      reduced_df['item_id'])),\n","      dtype='int8'\n","    ).tocsr()\n","\n","  model = implicit.als.AlternatingLeastSquares(\n","      iterations = iterations,\n","      factors=factors,\n","      regularization=regularization,\n","      dtype=np.float32\n","  )\n","\n","  users_to_keep = get_users_to_keep(reduced_df, n=minimum_clicks)\n","  items_to_exclude = get_items_to_exclude(reduced_df, proportion=popularity_threshold)\n","\n","  user_item_train = user_item[users_to_keep, :]\n","\n","  model.fit(user_item_train, show_progress=True)\n","\n","  args = {'userid' : validation_user_ids,\n","          'user_items' : user_item[validation_user_ids,:],\n","          'filter_items' : items_to_exclude,\n","          'filter_already_liked_items' : False,\n","          'recalculate_user' : True,\n","          'N' : n_recs\n","          }\n","\n","  recs = model.recommend(**args)\n","\n","  recs = pd.DataFrame(data={'session' : reduced_df.loc[reduced_df['dataset'] == 'test']['session'].unique(),\n","                                'aid' : recs[0][:].tolist(),\n","                                  'confidence' : recs[1][:].tolist()})\n","  recs = recs.set_index('session').apply(pd.Series.explode).reset_index()\n","  recs['aid'] = recs['aid'].map(item_ids)\n","  recs['n'] = recs.groupby('session').cumcount() + 1\n","  return recs"],"metadata":{"id":"Y09HWhBm7Bc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if als:\n","  train = get_train(validation, sample_prop, columns=['session', 'aid'])\n","  als_recs = get_als_recommendations(train, reduced_df, 200)\n","\n","  save_parquet(als_recs, f'{path_to_candidate_features}/als', files=100, split_column = 'session')"],"metadata":{"id":"kt8ZSrDWgp1Q"},"execution_count":null,"outputs":[]}]}