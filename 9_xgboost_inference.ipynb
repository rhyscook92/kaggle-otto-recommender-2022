{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_nEfIV7BVI6ZbF6L_iJkjPV0CeGLPX2Y","timestamp":1674859023080},{"file_id":"1eA9IPu0ct5BBp8Z8775IN8wRL0-8ECc6","timestamp":1672445355725},{"file_id":"1Y9trOoIWI7h8XxAt4XWoxIajg8p2hXE0","timestamp":1670368163491}],"machine_shape":"hm","authorship_tag":"ABX9TyM7KUBqvSrgTp5jdrGngGq6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Use the xgboost models to score each of the 200 candidates #"],"metadata":{"id":"t9bc6KmNE4xq"}},{"cell_type":"code","source":["local = False\n","if local:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  %cd /content/drive/MyDrive/'Kaggle Otto Reccommender'/data\n","  path_to_module = '/content/drive/MyDrive/Kaggle Otto Reccommender/'\n","else:\n","  !mkdir /my_mnt_dir\n","  !google-drive-ocamlfuse /my_mnt_dir\n","  %cd /my_mnt_dir/'Kaggle Otto Reccommender'/data\n","  path_to_module = '/my_mnt_dir/Kaggle Otto Reccommender/'\n","\n","import sys    \n","sys.path.append(path_to_module)"],"metadata":{"id":"rgw3DUBDx7hN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install xgboost --upgrade\n","!pip install fastparquet\n","!pip install polars"],"metadata":{"id":"X1mfpfJmv4y6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","import numpy as np\n","import pandas as pd\n","import gc\n","from tqdm import tqdm\n","from copy import deepcopy\n","import polars as pl\n","\n","import xgboost as xgb\n","from otto_utils import create_sub"],"metadata":{"id":"bbpCdPcQOMqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path_to_training_data = './test_training_data'\n","training_data = pl.read_parquet(f'{path_to_training_data}/training_data.parquet')\n","training_data = training_data.to_pandas()\n","\n","for column in training_data.columns:\n","  if training_data[column].dtype in ('Int32', 'float64', 'Int16'):\n","    training_data[column] = training_data[column].astype('float32')\n","training_data.replace([np.inf, -np.inf], np.NaN, inplace=True)"],"metadata":{"id":"mU6CXtM5f3xL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Xgboost inference functions ##"],"metadata":{"id":"CmIOqM0-4nl_"}},{"cell_type":"code","source":["def inference(dtrain_list, recall_type, folds, features):\n","  for fold in range(0, folds):\n","    if fold == 0:\n","      preds = collect_preds(dtrain_list, recall_type, fold, features)\n","    else:\n","      preds += collect_preds(dtrain_list, recall_type, fold, features)\n","  preds = preds / folds\n","\n","  return preds\n","\n","def collect_preds(dtrain_list, recall_type, fold, features):\n","  for i, dtrain_item in enumerate(tqdm(dtrain_list)):\n","    groups = dtrain_item.groupby('session', as_index=False).agg({'aid' : 'count'}).aid.values.tolist()\n","    dtrain = xgb.DMatrix(dtrain_item[features])\n","    dtrain.set_group(groups)\n","    model = xgb.Booster()\n","    model.load_model(f'../models/xgb_models/{recall_type}_{fold}.xgb')\n","    preds_chunk = model.predict(dtrain)\n","    if i == 0:\n","      preds = preds_chunk\n","    else:\n","      preds = np.append(preds, preds_chunk)\n","\n","  return preds\n","\n","def get_top_20_df(preds, training_data, return_all=True):\n","  predictions = training_data[['session','aid']].copy()\n","  predictions['preds'] = preds\n","  predictions.sort_values(by=['session', 'preds'], ascending=[True, False], inplace=True)\n","  predictions['n'] = predictions.groupby('session').cumcount() + 1\n","  if return_all:\n","    return predictions\n","  else:\n","    return predictions.loc[predictions['n'] <= 20]"],"metadata":{"id":"aHItK8SDkdkf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aids_and_sessions = training_data[['session','aid']]\n","features = [feature for feature in training_data.columns if feature not in ['session', 'aid', 'percent_of_test_weeks_interacted']]\n","dtrain_list = []\n","sessions = training_data['session'].unique()\n","session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), 10 ) ]\n","\n","for session_list in tqdm(session_lists):\n","  dtrain_list.append(training_data.query(f'session in {session_list}').copy())\n","  training_data = training_data.loc[~training_data.session.isin(session_list)]\n","\n","folds=5\n","del training_data"],"metadata":{"id":"t4-2mcw0231V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clicks"],"metadata":{"id":"hI8eh216E9vT"}},{"cell_type":"code","source":["click_preds = inference(dtrain_list, 'clicks', folds=folds, features=features)\n","clicks_df = get_top_20_df(click_preds, aids_and_sessions, return_all=True)\n","clicks_df['n'] = clicks_df['n'].astype('int16')\n","subset = clicks_df.loc[:, ['session', 'aid', 'n']]\n","subset = subset.reset_index(drop=True)\n","subset.to_feather(f'{path_to_training_data}/xgb_clicks_df.feather')\n","clicks_df = clicks_df.loc[clicks_df['n'] <= 20]"],"metadata":{"id":"mhFef2nuRciC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Carts ##"],"metadata":{"id":"05ZXUO5cE-4h"}},{"cell_type":"code","source":["cart_preds = inference(dtrain_list, 'carts', folds=folds, features=features)\n","carts_df = get_top_20_df(cart_preds, aids_and_sessions, return_all=True)\n","carts_df['n'] = carts_df['n'].astype('int16')\n","subset = carts_df.loc[:, ['session', 'aid', 'n']]\n","subset = subset.reset_index(drop=True)\n","subset.to_feather(f'{path_to_training_data}/xgb_carts_df.feather')\n","carts_df = carts_df.loc[carts_df['n'] <= 20]"],"metadata":{"id":"DL7a9eFldGRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Orders"],"metadata":{"id":"tV1bHZeJFBN7"}},{"cell_type":"code","source":["order_preds = inference(dtrain_list, 'orders', folds=folds, features=features)\n","orders_df = get_top_20_df(order_preds, aids_and_sessions, return_all=True)\n","orders_df['n'] = orders_df['n'].astype('int16')\n","subset = orders_df.loc[:, ['session', 'aid', 'n']]\n","subset = subset.reset_index(drop=True)\n","subset.to_feather(f'{path_to_training_data}/xgb_orders_df.feather')\n","orders_df = orders_df.loc[orders_df['n'] <= 20]"],"metadata":{"id":"jPuw-0FIf2E4"},"execution_count":null,"outputs":[]}]}