{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNSoRGgKYL5bDzuwHVceDPy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Create covisitation matrices to be used in candidate generation #\n","We create three types of covisitation matrix:\n","1. What else did people who clicked/carted/ordered the focal product also click/cart/order weighted to those occuring more recently in the data\n","2. What else did people who clicked/carted/ordered the focal product also click/cart/order weighted to items being carted and ordered. \n","3. What else did people who carted/ordered the product also cart/order. \n","\n","The joining operations are memory intensive so we split the dataframe into chunks and work through piecewise to avoid running out of ram.\n","\n","This process is inspired by this kaggle post: https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575"],"metadata":{"id":"1SdiYblJx2ao"}},{"cell_type":"code","source":["local = True\n","if local:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  %cd /content/drive/MyDrive/'Kaggle Otto Reccommender'/data\n","  path_to_module = '/content/drive/MyDrive/Kaggle Otto Reccommender/'\n","else:\n","  !mkdir /my_mnt_dir\n","  !google-drive-ocamlfuse /my_mnt_dir\n","  %cd /my_mnt_dir/'Kaggle Otto Reccommender'/data\n","  path_to_module = '/my_mnt_dir/Kaggle Otto Reccommender/'\n","\n","import sys    \n","sys.path.append(path_to_module)"],"metadata":{"id":"rgw3DUBDx7hN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import glob\n","import numpy as np\n","import pandas as pd\n","import gc\n","from tqdm import tqdm\n","from otto_utils import get_train, get_test, convert_columns, make_directory\n","import os"],"metadata":{"id":"bbpCdPcQOMqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fastparquet"],"metadata":{"id":"ExzF1gWlEUUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_prop = None\n","validation = False\n","path_to_candidate_features = './train_candidate_features' if validation else './test_candidate_features'\n","make_directory(f'{path_to_candidate_features}/covisitation_parquet')\n","make_directory(f'{path_to_candidate_features}/cart_order_parquet')\n","make_directory(f'{path_to_candidate_features}/also_buy_parquet')\n","n=20"],"metadata":{"id":"wJuOKZmKTHVb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reduced_df = pd.concat([get_train(validation=validation, sample_prop=sample_prop), get_test(validation=validation, sample_prop=sample_prop)])\n","reduced_df['ts'] = reduced_df['ts'] / 1000\n","reduced_df['ts'] = reduced_df['ts'].astype('int32')"],"metadata":{"id":"DbyydmmcNB80"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reduced_df = convert_columns(reduced_df)"],"metadata":{"id":"FjqDOBh6ttXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sessions = reduced_df['session'].unique()\n","sessions.sort()\n","aids = reduced_df['aid'].unique()\n","aids.sort()\n","\n","session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), int(reduced_df.shape[0]*200 / 163955180)) ]\n","aid_lists = [np_array.tolist() for np_array in np.array_split(np.array(aids), int(reduced_df.shape[0]*100 / 163955180)) ]\n","max_ts = 1662328791\n","min_ts = 1659304800\n","diff_ts = max_ts - min_ts"],"metadata":{"id":"BE56LzDP714K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted_chunks = []\n","for session_list in tqdm(session_lists):\n","  chunk = reduced_df[(reduced_df['session'] >= min(session_list)) & (reduced_df['session'] <= max(session_list))]\n","  chunk = chunk.sort_values(['session','ts'],ascending=[True,False])\n","  chunk = chunk.reset_index(drop=True)\n","  chunk['n'] = chunk.groupby('session').cumcount()\n","  chunk = chunk.loc[chunk.n<30].drop('n',axis=1)\n","  sorted_chunks.append(chunk)\n","reduced_df = pd.concat(sorted_chunks)\n","del sorted_chunks"],"metadata":{"id":"2hGT0nQQv3D_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the clicks/cart/order to clicks/cart/order covisitation matrix weighted towards things happening more recently. "],"metadata":{"id":"Whtc1gRL7Vyh"}},{"cell_type":"code","source":["for i, aid_list in enumerate(tqdm(aid_lists)):\n","  tmp_list = []\n","\n","  for session_list in session_lists:\n","    df = reduced_df[(reduced_df['session'] >= min(session_list)) & (reduced_df['session'] <= max(session_list))]\n","    tmp = (\n","        df.loc[(df['aid'] >= min(aid_list)) & (df['aid'] <= max(aid_list))]\n","        .merge(df,\n","              how = 'inner',\n","              on = 'session')\n","    )\n","    tmp = (\n","        tmp\n","        .loc[ ((tmp.ts_x - tmp.ts_y).abs() < 24 * 60 * 60) & (tmp.aid_x != tmp.aid_y) ]\n","        .drop_duplicates(['session', 'aid_x', 'aid_y'])\n","    )\n","    tmp['wgt'] = 1 + 3 * (tmp.ts_x - min_ts) / diff_ts\n","    tmp = (\n","        tmp.groupby(['aid_x', 'aid_y'], as_index=False)\n","        .agg({'wgt' : 'sum'})\n","        .rename(columns={'wgt' : 'pairings'})\n","    )\n","    tmp['pairings'] = tmp['pairings'].astype('float32')\n","    tmp_list.append(tmp)\n","  out = pd.concat(tmp_list)\n","  out = (\n","      pd.concat(tmp_list)\n","      .groupby(['aid_x', 'aid_y'], as_index=False)\n","      .agg({'pairings' : 'sum'})\n","      .sort_values(by=['aid_x', 'pairings'], ascending=[True, False])\n","  )\n","\n","  out['n'] = out.groupby(['aid_x']).cumcount() + 1\n","  out = out.loc[out['n'] <= n]\n","  for column in ['aid_x', 'aid_y']:\n","    out[column] = out[column].astype('int32')\n","  out.to_parquet(f'{path_to_candidate_features}covisitation_parquet/wgt_covisitation_{i}_top{n}.parquet', index=False)\n","  del tmp_list, out"],"metadata":{"id":"ioT9gteQTZBv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the clicks/cart/order to clicks/cart/order weighted towards carts/orders matrix weighted by the type of interaction"],"metadata":{"id":"kWfk0ELK7emM"}},{"cell_type":"code","source":["type_weight_map = {\n","    'clicks' : 1,\n","    'carts' : 6,\n","    'orders' : 3\n","}\n","\n","for i, aid_list in enumerate(tqdm(aid_lists)):\n","  tmp_list = []\n","\n","  for session_list in session_lists:\n","    df = reduced_df[(reduced_df['session'] >= min(session_list)) & (reduced_df['session'] <= max(session_list))]\n","    tmp = (\n","        df.loc[(df['aid'] >= min(aid_list)) & (df['aid'] <= max(aid_list))]\n","        .merge(df,\n","              how = 'inner',\n","              on = 'session')\n","    )\n","    tmp = (\n","        tmp\n","        .loc[ ((tmp.ts_x - tmp.ts_y).abs() < 24 * 60 * 60) & (tmp.aid_x != tmp.aid_y) ]\n","    )\n","    tmp['wgt'] = tmp['type_y'].map(type_weight_map)\n","    tmp = (\n","        tmp.groupby(['aid_x', 'aid_y'], as_index=False)\n","        .agg({'wgt' : 'sum'})\n","        .rename(columns={'wgt' : 'pairings'})\n","    )\n","    tmp['pairings'] = tmp['pairings'].astype('int32')\n","    tmp_list.append(tmp)\n","  out = pd.concat(tmp_list)\n","  out = (\n","      pd.concat(tmp_list)\n","      .groupby(['aid_x', 'aid_y'], as_index=False)\n","      .agg({'pairings' : 'sum'})\n","      .sort_values(by=['aid_x', 'pairings'], ascending=[True, False])\n","  )\n","\n","  out['n'] = out.groupby(['aid_x']).cumcount() + 1\n","  out = out.loc[out['n'] <= 15]\n","\n","  for column in ['aid_x', 'aid_y']:\n","    out[column] = out[column].astype('int32')\n","  out.to_parquet(f'{path_to_candidate_features}/cart_order_parquet/cart_order_top15_{i}.parquet', index=False)\n","  \n","  del tmp_list, out"],"metadata":{"id":"Z6vjqfnFSvYw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the covisitation of what people carted/ordered alongside carts and orders. E.g those that bought x also bought... "],"metadata":{"id":"9BI2NtlG8Yuu"}},{"cell_type":"code","source":["session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), int(reduced_df.shape[0]*3 / 163955180)) ]\n","aid_lists = [np_array.tolist() for np_array in np.array_split(np.array(aids), int(reduced_df.shape[0]*10 / 163955180)) ]"],"metadata":{"id":"K1-ENh39FbfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, aid_list in enumerate(tqdm(aid_lists)):\n","  tmp_list = []\n","\n","  for session_list in session_lists:\n","    df = reduced_df[(reduced_df['session'] >= min(session_list)) & (reduced_df['session'] <= max(session_list))]\n","    df = df.loc[df['type'].isin(['carts', 'orders'])]\n","    tmp = (\n","        df.loc[(df['aid'] >= min(aid_list)) & (df['aid'] <= max(aid_list))]\n","        .merge(df,\n","              how = 'inner',\n","              on = 'session')\n","    )\n","    tmp = (\n","        tmp\n","        .loc[ ((tmp.ts_x - tmp.ts_y).abs() < 14 * 24 * 60 * 60) & (tmp.aid_x != tmp.aid_y) ]\n","    )\n","    tmp['wgt'] = 1\n","    tmp = (\n","        tmp.groupby(['aid_x', 'aid_y'], as_index=False)\n","        .agg({'wgt' : 'sum'})\n","        .rename(columns={'wgt' : 'pairings'})\n","    )\n","    tmp['pairings'] = tmp['pairings'].astype('int32')\n","    tmp_list.append(tmp)\n","  out = pd.concat(tmp_list)\n","  out = (\n","      pd.concat(tmp_list)\n","      .groupby(['aid_x', 'aid_y'], as_index=False)\n","      .agg({'pairings' : 'sum'})\n","      .sort_values(by=['aid_x', 'pairings'], ascending=[True, False])\n","  )\n","\n","  out['n'] = out.groupby(['aid_x']).cumcount() + 1\n","  out = out.loc[out['n'] <= 15]\n","\n","  for column in ['aid_x', 'aid_y']:\n","    out[column] = out[column].astype('int32')\n","  out.to_parquet(f'{path_to_candidate_features}/also_buy_parquet/also_buy_top15_{i}.parquet', index=False)\n","  \n","  del tmp_list, out"],"metadata":{"id":"it_GRoK8cjhC"},"execution_count":null,"outputs":[]}]}